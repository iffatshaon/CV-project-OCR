{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: Dataset/Bangla/Dataset/Train\n",
      "Checked 12000 images in total.\n",
      "Removed 0 unreadable images.\n",
      "Removed 0 non-image files.\n",
      "Total number of images: 12000\n",
      "Total number of labels: 50\n",
      "\n",
      "Directory: Dataset/Bangla/Dataset/Test\n",
      "Checked 3000 images in total.\n",
      "Removed 0 unreadable images.\n",
      "Removed 0 non-image files.\n",
      "Total number of images: 3000\n",
      "Total number of labels: 50\n",
      "\n",
      "Directory: Dataset/English/data/training_data\n",
      "Checked 20628 images in total.\n",
      "Removed 0 unreadable images.\n",
      "Removed 0 non-image files.\n",
      "Total number of images: 20628\n",
      "Total number of labels: 36\n",
      "\n",
      "Directory: Dataset/English/data/testing_data\n",
      "Checked 1008 images in total.\n",
      "Removed 0 unreadable images.\n",
      "Removed 0 non-image files.\n",
      "Total number of images: 1008\n",
      "Total number of labels: 36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combined.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from preprocess import preprocess_image  # Import from preprocess.py\n",
    "from model import OCRModel  # Import from separate.py\n",
    "from dataload import bangla_load, english_load\n",
    "\n",
    "# Load Bangla and English data loaders\n",
    "bangla_train_loader, bangla_val_loader, bangla_test_loader, bangla_num_classes = bangla_load\n",
    "english_train_loader, english_val_loader, english_test_loader, english_num_classes = english_load\n",
    "\n",
    "# Load pre-trained models with weights_only=True\n",
    "bangla_model = OCRModel(bangla_num_classes)\n",
    "english_model = OCRModel(english_num_classes)\n",
    "bangla_model.load_state_dict(torch.load(\"saved_models/bangla_model.pth\", weights_only=True)['model_state_dict'])\n",
    "english_model.load_state_dict(torch.load(\"saved_models/english_model.pth\", weights_only=True)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze layers in both models up to the LSTM layers\n",
    "for param in list(bangla_model.parameters())[:-4]:\n",
    "    param.requires_grad = True\n",
    "for param in list(english_model.parameters())[:-4]:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique labels: 86\n",
      "Total number of unique labels: 86\n",
      "Number of classes: 86\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Data should be a list of tuples (features, labels)\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, labels = self.data[idx]\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, bangla_loader, english_loader):\n",
    "        # Modify labels for both datasets to make them unique\n",
    "        self.bangla_data = self.modify_label('b', bangla_loader)\n",
    "        self.english_data = self.modify_label('e', english_loader)\n",
    "\n",
    "        # Combine the datasets into a single dataset\n",
    "        self.combined_data = self.bangla_data + self.english_data\n",
    "\n",
    "        # Calculate unique labels\n",
    "        self.unique_labels = self.calculate_unique_labels()\n",
    "        print(f\"Total number of unique labels: {len(self.unique_labels)}\")\n",
    "\n",
    "        # Create a label-to-index mapping\n",
    "        self.label_mapping = self.create_label_mapping()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total length of the combined dataset\n",
    "        return len(self.combined_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch item from the combined dataset\n",
    "        features, label = self.combined_data[idx]\n",
    "        \n",
    "        # Ensure features are tensors\n",
    "        features = torch.tensor(features) if not isinstance(features, torch.Tensor) else features\n",
    "        \n",
    "        # Convert label to integer using the mapping\n",
    "        label = self.label_mapping[label]\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        return features, label\n",
    "\n",
    "    def modify_label(self, prefix, dataloader):\n",
    "        # Create a new dataset with modified labels\n",
    "        new_data = []\n",
    "        for data, label in dataloader:\n",
    "            # Add prefix to each label (e.g., 'b0', 'e1')\n",
    "            modified_label = [f\"{prefix}{lbl}\" for lbl in label]\n",
    "            # Append data and unique labels\n",
    "            new_data.extend(zip(data, modified_label))\n",
    "        return new_data\n",
    "    \n",
    "    def create_label_mapping(self):\n",
    "        # Extract unique labels\n",
    "        unique_labels = {item[1] for item in self.combined_data}\n",
    "        # Create a mapping from label string to integer\n",
    "        return {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "    def calculate_unique_labels(self):\n",
    "        # Extract unique labels from the combined data\n",
    "        labels = [item[1] for item in self.combined_data]\n",
    "        return set(labels)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        # Return the total number of unique classes\n",
    "        return len(self.unique_labels)\n",
    "\n",
    "# Combine datasets\n",
    "combined_dataset = CombinedDataset(bangla_train_loader, english_train_loader)\n",
    "combined_dataloader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Combine datasets\n",
    "combined_test_dataset = CombinedDataset(bangla_test_loader, english_test_loader)\n",
    "combined_test_dataloader = DataLoader(combined_test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Get the number of classes\n",
    "print(f\"Number of classes: {combined_dataset.get_num_classes()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 24796\n",
      "Validation set size: 6200\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "def split_dataloader(dataloader, train_ratio=0.8):\n",
    "    \n",
    "    # Access the dataset from the DataLoader\n",
    "    dataset = dataloader.dataset\n",
    "    batch_size = dataloader.batch_size\n",
    "\n",
    "    # Calculate sizes for train and validation datasets\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = total_size - train_size\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoaders for the subsets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "combined_train_loader, combined_val_loader = split_dataloader(combined_dataloader, train_ratio=0.8)\n",
    "\n",
    "# Check sizes of datasets\n",
    "print(f\"Training set size: {len(combined_train_loader.dataset)}\")\n",
    "print(f\"Validation set size: {len(combined_val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, bangla_model, english_model, combined_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.bangla_feature_extractor = nn.Sequential(*list(bangla_model.children())[:-2])  # Up to LSTM\n",
    "        self.bangla_lstm = list(bangla_model.children())[-2]  # LSTM layer\n",
    "        self.english_feature_extractor = nn.Sequential(*list(english_model.children())[:-2])  # Up to LSTM\n",
    "        self.english_lstm = list(english_model.children())[-2]  # LSTM layer\n",
    "\n",
    "        \n",
    "        # Joint layers\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, combined_classes)\n",
    "\n",
    "    def forward(self, image_x):\n",
    "        # Bangla feature extraction\n",
    "        bangla_features = self.bangla_feature_extractor(image_x)  # 4D output\n",
    "        bangla_features = bangla_features.permute(0, 2, 3, 1).reshape(bangla_features.size(0), -1, 1024)\n",
    "        bangla_features, _ = self.bangla_lstm(bangla_features)  # LSTM processing\n",
    "        \n",
    "        # English feature extraction\n",
    "        english_features = self.english_feature_extractor(image_x)  # 4D output\n",
    "        english_features = english_features.permute(0, 2, 3, 1).reshape(english_features.size(0), -1, 1024)\n",
    "        english_features, _ = self.english_lstm(english_features)  # LSTM processing\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((bangla_features[:, -1, :], english_features[:, -1, :]), dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(combined_features))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, lr=0.001):\n",
    "        \n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.lr = lr\n",
    "\n",
    "        # Determine device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Assign model and move it to the device\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train_and_validate(self, epochs=10):\n",
    "        \n",
    "        history = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_accuracy\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_accuracy\": []\n",
    "        }\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            for images, labels in self.train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Training accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            train_loss = running_loss / len(self.train_loader)\n",
    "            train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in self.val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Validation accuracy\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loss = val_loss / len(self.val_loader)\n",
    "            val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "            # Append metrics to history\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"train_accuracy\"].append(train_accuracy)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        return history\n",
    "\n",
    "    def test(self):\n",
    "        \n",
    "        self.model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                # Test accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        test_loss = test_loss / len(self.test_loader)\n",
    "        test_accuracy = 100 * correct_test / total_test\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize combined model\n",
    "combined_classes = combined_dataset.get_num_classes()\n",
    "combined_model = CombinedModel(bangla_model, english_model, combined_classes)\n",
    "\n",
    "# Define training settings for the combined model\n",
    "combined_optimizer = optim.Adam(filter(lambda p: p.requires_grad, combined_model.parameters()), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.7622, Train Accuracy: 28.00%, Val Loss: 1.9742, Val Accuracy: 43.63%\n",
      "Epoch 2: Train Loss: 1.6232, Train Accuracy: 53.14%, Val Loss: 1.3964, Val Accuracy: 58.26%\n",
      "Epoch 3: Train Loss: 1.2335, Train Accuracy: 63.18%, Val Loss: 1.3562, Val Accuracy: 58.10%\n",
      "Epoch 4: Train Loss: 1.0878, Train Accuracy: 67.22%, Val Loss: 1.0598, Val Accuracy: 67.73%\n",
      "Epoch 5: Train Loss: 0.8792, Train Accuracy: 72.92%, Val Loss: 0.9159, Val Accuracy: 70.82%\n",
      "Epoch 6: Train Loss: 0.7981, Train Accuracy: 75.16%, Val Loss: 0.8818, Val Accuracy: 72.48%\n",
      "Epoch 7: Train Loss: 0.7275, Train Accuracy: 77.46%, Val Loss: 0.7860, Val Accuracy: 75.39%\n",
      "Epoch 8: Train Loss: 0.6570, Train Accuracy: 79.42%, Val Loss: 0.7397, Val Accuracy: 76.44%\n",
      "Epoch 9: Train Loss: 0.5888, Train Accuracy: 81.57%, Val Loss: 0.7093, Val Accuracy: 77.39%\n",
      "Epoch 10: Train Loss: 0.5624, Train Accuracy: 82.49%, Val Loss: 0.7159, Val Accuracy: 77.94%\n",
      "Epoch 11: Train Loss: 0.4976, Train Accuracy: 84.22%, Val Loss: 0.6435, Val Accuracy: 79.65%\n",
      "Epoch 12: Train Loss: 0.4567, Train Accuracy: 85.38%, Val Loss: 0.6217, Val Accuracy: 80.50%\n",
      "Epoch 13: Train Loss: 0.4260, Train Accuracy: 86.47%, Val Loss: 0.5998, Val Accuracy: 81.56%\n",
      "Epoch 14: Train Loss: 0.4035, Train Accuracy: 87.14%, Val Loss: 0.5673, Val Accuracy: 83.08%\n",
      "Epoch 15: Train Loss: 0.3503, Train Accuracy: 88.97%, Val Loss: 0.5535, Val Accuracy: 83.50%\n",
      "Epoch 16: Train Loss: 0.3292, Train Accuracy: 89.31%, Val Loss: 0.5571, Val Accuracy: 83.29%\n",
      "Epoch 17: Train Loss: 0.3033, Train Accuracy: 90.07%, Val Loss: 0.5252, Val Accuracy: 84.13%\n",
      "Epoch 18: Train Loss: 0.2737, Train Accuracy: 91.09%, Val Loss: 0.5383, Val Accuracy: 83.69%\n",
      "Epoch 19: Train Loss: 0.2723, Train Accuracy: 91.16%, Val Loss: 0.5115, Val Accuracy: 84.40%\n",
      "Epoch 20: Train Loss: 0.2249, Train Accuracy: 92.67%, Val Loss: 0.5025, Val Accuracy: 85.05%\n",
      "Epoch 21: Train Loss: 0.2204, Train Accuracy: 92.82%, Val Loss: 0.4825, Val Accuracy: 85.27%\n",
      "Epoch 22: Train Loss: 0.1885, Train Accuracy: 93.87%, Val Loss: 0.4823, Val Accuracy: 85.73%\n",
      "Epoch 23: Train Loss: 0.1739, Train Accuracy: 94.45%, Val Loss: 0.4875, Val Accuracy: 85.85%\n",
      "Epoch 24: Train Loss: 0.1682, Train Accuracy: 94.67%, Val Loss: 0.5119, Val Accuracy: 85.06%\n",
      "Epoch 25: Train Loss: 0.1663, Train Accuracy: 94.72%, Val Loss: 0.4989, Val Accuracy: 85.74%\n",
      "Epoch 26: Train Loss: 0.1398, Train Accuracy: 95.42%, Val Loss: 0.4478, Val Accuracy: 87.16%\n",
      "Epoch 27: Train Loss: 0.1369, Train Accuracy: 95.69%, Val Loss: 0.4773, Val Accuracy: 86.79%\n",
      "Epoch 28: Train Loss: 0.1238, Train Accuracy: 96.17%, Val Loss: 0.4828, Val Accuracy: 86.31%\n",
      "Epoch 29: Train Loss: 0.1100, Train Accuracy: 96.55%, Val Loss: 0.4572, Val Accuracy: 87.15%\n",
      "Epoch 30: Train Loss: 0.0996, Train Accuracy: 96.76%, Val Loss: 0.4788, Val Accuracy: 86.69%\n",
      "Epoch 31: Train Loss: 0.0904, Train Accuracy: 97.20%, Val Loss: 0.4697, Val Accuracy: 86.79%\n",
      "Epoch 32: Train Loss: 0.0895, Train Accuracy: 97.14%, Val Loss: 0.4645, Val Accuracy: 86.98%\n",
      "Epoch 33: Train Loss: 0.0882, Train Accuracy: 97.23%, Val Loss: 0.4921, Val Accuracy: 86.50%\n",
      "Epoch 34: Train Loss: 0.0867, Train Accuracy: 97.23%, Val Loss: 0.4723, Val Accuracy: 87.42%\n",
      "Epoch 35: Train Loss: 0.0841, Train Accuracy: 97.19%, Val Loss: 0.4530, Val Accuracy: 87.95%\n",
      "Epoch 36: Train Loss: 0.0575, Train Accuracy: 98.19%, Val Loss: 0.4490, Val Accuracy: 88.52%\n",
      "Epoch 37: Train Loss: 0.0744, Train Accuracy: 97.54%, Val Loss: 0.5011, Val Accuracy: 87.08%\n",
      "Epoch 38: Train Loss: 0.0817, Train Accuracy: 97.18%, Val Loss: 0.4586, Val Accuracy: 88.21%\n",
      "Epoch 39: Train Loss: 0.0664, Train Accuracy: 97.92%, Val Loss: 0.4734, Val Accuracy: 87.81%\n",
      "Epoch 40: Train Loss: 0.0620, Train Accuracy: 97.98%, Val Loss: 0.5327, Val Accuracy: 86.84%\n",
      "Epoch 41: Train Loss: 0.0692, Train Accuracy: 97.65%, Val Loss: 0.4561, Val Accuracy: 88.73%\n",
      "Epoch 42: Train Loss: 0.0437, Train Accuracy: 98.61%, Val Loss: 0.4444, Val Accuracy: 89.18%\n",
      "Epoch 43: Train Loss: 0.0591, Train Accuracy: 98.01%, Val Loss: 0.4948, Val Accuracy: 88.15%\n",
      "Epoch 44: Train Loss: 0.0673, Train Accuracy: 97.74%, Val Loss: 0.4538, Val Accuracy: 88.95%\n",
      "Epoch 45: Train Loss: 0.0642, Train Accuracy: 97.79%, Val Loss: 0.4925, Val Accuracy: 87.76%\n",
      "Epoch 46: Train Loss: 0.0518, Train Accuracy: 98.26%, Val Loss: 0.5209, Val Accuracy: 87.69%\n",
      "Epoch 47: Train Loss: 0.0415, Train Accuracy: 98.60%, Val Loss: 0.4618, Val Accuracy: 88.94%\n",
      "Epoch 48: Train Loss: 0.0388, Train Accuracy: 98.71%, Val Loss: 0.4758, Val Accuracy: 88.55%\n",
      "Epoch 49: Train Loss: 0.0738, Train Accuracy: 97.51%, Val Loss: 0.4745, Val Accuracy: 88.77%\n",
      "Epoch 50: Train Loss: 0.0361, Train Accuracy: 98.76%, Val Loss: 0.4782, Val Accuracy: 88.92%\n",
      "Test Loss: 1.0511, Test Accuracy: 81.01%\n"
     ]
    }
   ],
   "source": [
    "# Common data\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "# Initialize the trainer\n",
    "combined_trainer = OCRTrainer(combined_model, combined_train_loader, combined_val_loader, combined_test_dataloader, lr=lr)\n",
    "\n",
    "# Train and validate\n",
    "combined_history = combined_trainer.train_and_validate(epochs=epochs)\n",
    "\n",
    "# Test\n",
    "test_loss, test_accuracy = combined_trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, test_loader, num_classes, output_dir=\"evaluation_outputs\"):\n",
    "        \"\"\"\n",
    "        Initializes the evaluator.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Trained model to evaluate.\n",
    "            test_loader (DataLoader): DataLoader for the test dataset.\n",
    "            num_classes (int): Number of classes in the dataset.\n",
    "            output_dir (str): Directory to save evaluation plots and metrics.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def evaluate_and_save(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on test data and saves visualizations and metrics to disk.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        all_images = []\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                # Collect all images, labels, and predictions\n",
    "                all_images.extend(images.cpu())  # Collect all test images\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "        # Convert collected data to NumPy arrays\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "        # Save sample predictions\n",
    "        self._save_sample_predictions(all_images, all_labels, all_predictions)\n",
    "\n",
    "        # Save confusion matrix\n",
    "        self._save_confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "        # Save classification report\n",
    "        self._save_classification_report(all_labels, all_predictions)\n",
    "\n",
    "        # Save ROC curve\n",
    "        self._save_roc_curve(all_labels, all_probabilities)\n",
    "\n",
    "    def _save_sample_predictions(self, images, labels, predictions):\n",
    "        \"\"\"\n",
    "        Randomly selects test images until there are images of 5 unique labels \n",
    "        and saves them along with their predictions and true labels to a file.\n",
    "        \"\"\"\n",
    "        max_samples = 5  # Limit the number of unique labels\n",
    "        unique_labels = {}\n",
    "        selected_indices = []\n",
    "        total_samples = len(labels)\n",
    "\n",
    "        # Randomly shuffle indices\n",
    "        indices = list(range(total_samples))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # Select images with unique labels\n",
    "        for idx in indices:\n",
    "            label = labels[idx]\n",
    "            if label not in unique_labels:\n",
    "                unique_labels[label] = True\n",
    "                selected_indices.append(idx)\n",
    "                if len(unique_labels) == max_samples:\n",
    "                    break\n",
    "\n",
    "        # Plot and save the selected samples\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            plt.subplot(1, len(selected_indices), i + 1)\n",
    "            plt.imshow(images[idx].squeeze(), cmap='gray')\n",
    "            plt.title(f\"True: {labels[idx]}\\nPred: {predictions[idx]}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        file_path = os.path.join(self.output_dir, \"sample_predictions.png\")\n",
    "        plt.savefig(file_path)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _save_confusion_matrix(self, labels, predictions):\n",
    "        \"\"\"\n",
    "        Saves the confusion matrix to a file.\n",
    "        \"\"\"\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        file_path = os.path.join(self.output_dir, \"confusion_matrix.png\")\n",
    "        plt.savefig(file_path)\n",
    "        plt.close()\n",
    "\n",
    "    def _save_classification_report(self, labels, predictions):\n",
    "        \"\"\"\n",
    "        Saves the classification report to a text file.\n",
    "        \"\"\"\n",
    "        report = classification_report(labels, predictions)\n",
    "        file_path = os.path.join(self.output_dir, \"classification_report.txt\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(report)\n",
    "\n",
    "    def _save_roc_curve(self, labels, probabilities):\n",
    "        # Binarize labels for multi-class ROC calculation\n",
    "        labels_binarized = label_binarize(labels, classes=range(self.num_classes))\n",
    "        fpr, tpr, _ = roc_curve(labels_binarized.ravel(), probabilities.ravel())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Plot the ROC curve\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, label=f\"Combined Classes (AUC = {roc_auc:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "        plt.title(\"ROC Curve (All Classes)\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        file_path = os.path.join(self.output_dir, \"roc_curve.png\")\n",
    "        plt.savefig(file_path)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bangla_evaluator = Evaluator(combined_model, combined_test_dataloader, combined_classes, output_dir=\"evaluation_outputs\")\n",
    "bangla_evaluator.evaluate_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
